{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5598a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input and output folders\n",
    "input_folder = r\"D:/GPM/Final/Original/Half_Hourly\"  # Replace with the actual path to the input folder\n",
    "output_folder = r\"D:/GPM/Final/Original/Daily\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "import netCDF4 as nc\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Specify the input and output folders\n",
    "input_folder = r\"D:/GPM/Final/Original/Half_Hourly\"  # Replace with your input folder path\n",
    "output_folder = r\"D:/GPM/Final/Original/Daily\"      # Replace with your output folder path\n",
    "\n",
    "# Define the start and end dates for the data period\n",
    "start_date = datetime.datetime(2022, 6, 1)\n",
    "end_date = datetime.datetime(2022, 6, 7)\n",
    "\n",
    "# List all files in the input folder\n",
    "LF = os.listdir(input_folder)\n",
    "print(\"Total files in LF:\", len(LF))\n",
    "\n",
    "# Read lat and lon from one of the input files\n",
    "example_file_path = os.path.join(input_folder, LF[0])\n",
    "with nc.Dataset(example_file_path) as example_dataset:\n",
    "    latitudes = example_dataset.variables['lat'][:]\n",
    "    longitudes = example_dataset.variables['lon'][:]\n",
    "\n",
    "# Process each day\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    start_time = current_date.replace(hour=3, minute=0)\n",
    "    end_time = (current_date + datetime.timedelta(days=1)).replace(hour=3, minute=0)\n",
    "    print(\"Start date and time:\", start_time, \"\\tEnd date and time:\", end_time)\n",
    "\n",
    "    # Filter files for the current day's range\n",
    "    LF2 = []\n",
    "    for file_name in LF:\n",
    "        match = re.match(r\"\\d{8}-S\\d{4}\\.nc\", file_name)\n",
    "        if match:\n",
    "            file_time_string = file_name.split('-')[1].split('.')[0]\n",
    "            file_time = datetime.datetime.strptime(file_time_string, \"S%H%M\")\n",
    "            file_date = datetime.datetime.strptime(file_name[:8], \"%Y%m%d\")\n",
    "            file_datetime = file_date.replace(hour=file_time.hour, minute=file_time.minute)\n",
    "            if start_time <= file_datetime < end_time:\n",
    "                LF2.append(file_name)\n",
    "\n",
    "    # Process the selected files\n",
    "    precip3D = []\n",
    "    for file_name in LF2:\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        try:\n",
    "            dataset = nc.Dataset(file_path)\n",
    "        except OSError as e:\n",
    "            print(\"Error opening file:\", file_path)\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Extract the variable data\n",
    "        data = dataset.variables['precipitation'][:]\n",
    "        precip3D.append(data)\n",
    "        dataset.close()\n",
    "\n",
    "    # Aggregate data if available\n",
    "    if precip3D:\n",
    "        precip3D = np.concatenate(precip3D, axis=0)\n",
    "        correction_factor = 0.5  # Half-hourly to daily conversion\n",
    "        precip3D_sum = np.sum(precip3D, axis=0) * correction_factor\n",
    "\n",
    "        # Ensure the array is correctly shaped (transpose if necessary)\n",
    "        if precip3D_sum.shape != (len(latitudes), len(longitudes)):\n",
    "            precip3D_sum = precip3D_sum.T  # Transpose the array\n",
    "\n",
    "        # Format the date for the output file\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        output_file_name = date_str + \".nc\"\n",
    "        output_file_path = os.path.join(output_folder, output_file_name)\n",
    "\n",
    "        # Create a new netCDF file and write the data\n",
    "        with nc.Dataset(output_file_path, \"w\", format=\"NETCDF4\") as dataset:\n",
    "            # Create dimensions\n",
    "            time_dim = dataset.createDimension(\"time\", 1)\n",
    "            lon_dim = dataset.createDimension(\"lon\", len(longitudes))\n",
    "            lat_dim = dataset.createDimension(\"lat\", len(latitudes))\n",
    "\n",
    "            # Create variables\n",
    "            time_var = dataset.createVariable(\"time\", np.float64, (\"time\",))\n",
    "            lon_var = dataset.createVariable(\"lon\", np.float32, (\"lon\",))\n",
    "            lat_var = dataset.createVariable(\"lat\", np.float32, (\"lat\",))\n",
    "            precip_var = dataset.createVariable(\"precipitationCal\", np.float32, (\"time\", \"lat\", \"lon\"), fill_value=-9999.9)\n",
    "\n",
    "            # Write data\n",
    "            time_var[:] = [0]  # Assuming a single time step\n",
    "            lon_var[:] = longitudes\n",
    "            lat_var[:] = latitudes\n",
    "            precip_var[0, :, :] = precip3D_sum\n",
    "\n",
    "            # Add attributes to the variables\n",
    "            time_var.standard_name = \"time\"\n",
    "            time_var.units = \"seconds since 1970-01-01 00:00:00 UTC\"\n",
    "            time_var.calendar = \"standard\"\n",
    "            time_var.axis = \"T\"\n",
    "\n",
    "            lon_var.standard_name = \"longitude\"\n",
    "            lon_var.long_name = \"longitude\"\n",
    "            lon_var.units = \"degrees_east\"\n",
    "            lon_var.axis = \"X\"\n",
    "\n",
    "            lat_var.standard_name = \"latitude\"\n",
    "            lat_var.long_name = \"latitude\"\n",
    "            lat_var.units = \"degrees_north\"\n",
    "            lat_var.axis = \"Y\"\n",
    "\n",
    "            precip_var.units = \"mm\"\n",
    "            precip_var.missing_value = -9999.9\n",
    "            precip_var.DimensionNames = \"time,lon,lat\"\n",
    "            precip_var.CodeMissingValue = -9999.9\n",
    "\n",
    "    else:\n",
    "        print(\"No data available for concatenation for\", date_str)\n",
    "\n",
    "    # Move to the next day\n",
    "    current_date += datetime.timedelta(days=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f00b10",
   "metadata": {},
   "source": [
    "Checking the data as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153cf340",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Daily\"\"\"\n",
    "import xarray as xr\n",
    "with xr.open_dataset(filename_or_obj= \"D:/GPM/Final/Original/Daily/20220601.nc\", engine='netcdf4') as file:\n",
    "    df = file.to_dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0169701",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Half Hourly\"\"\"\n",
    "import xarray as xr\n",
    "with xr.open_dataset(filename_or_obj= \"D:/GPM/Final/Original/Half_Hourly/20220601-S0000.nc\", engine='netcdf4') as file:\n",
    "    df = file.to_dataframe()\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
